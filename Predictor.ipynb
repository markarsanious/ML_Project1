{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#read data using pandas\n",
    "data = pd.read_csv(\"sp500_27270.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering data satisfying correlation >=0.95 or <=-0.95\n",
    "filtered_companies = []\n",
    "correlation = data.corr()\n",
    "for i in range(len(correlation)):\n",
    "    if(correlation['SP500'][i]>=0.95 or correlation['SP500'][i]<=-0.95):\n",
    "        filtered_companies.append(correlation['SP500'].keys().values[i])\n",
    "\n",
    "#extracting the columns of the companies satisfying the correlation limits\n",
    "filtered_data = data.filter(filtered_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the inputs (columns of companies stock prices)\n",
    "#specifying our output as the SP500 index column\n",
    "Y = np.array(filtered_data['SP500'])\n",
    "X = np.array(filtered_data.drop(columns=['SP500']))\n",
    "\n",
    "#Normalizing Data (using mean and standard deviation)\n",
    "X_scaled = preprocessing.scale(X)\n",
    "Y_scaled = preprocessing.scale(Y)\n",
    "\n",
    "#divide the dataset into training and testing sets in the ratio 80:20\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,Y_scaled,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'hidden_layer_sizes': (10, 2), 'solver': 'adam'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9808503659585882"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the MLP regressor with max iterations\n",
    "regressor = MLPRegressor(max_iter=3000)\n",
    "\n",
    "#specifying the different parameters to perform the grid search on\n",
    "param_grid = {'hidden_layer_sizes': [(15, 2), (10, 2)], 'activation': ['relu', 'tanh'], 'solver':['sgd', 'adam']}\n",
    "\n",
    "#perform gridsearch on all possible combinations of parameters with cross validation division \n",
    "clf = GridSearchCV(regressor, param_grid=param_grid, cv=3)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "#print the best architecture parameters and the score\n",
    "print(clf.best_params_)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dataframe by the SP500 index value descendingly\n",
    "sorted_dataframe = correlation.sort_values(['SP500'], ascending=False)\n",
    "\n",
    "#drop the value of the first row (one with the maximum SP500 value in the SP500 column) \n",
    "#since this represents the correlation of SP500 with itself. We actually looking for the most correlated company\n",
    "#which will be the second highest value in the SP500 index column\n",
    "sorted_dataframe = sorted_dataframe[1:]\n",
    "\n",
    "#extract the company name (the key) of this maximum value\n",
    "maximum_company = sorted_dataframe['SP500'].keys().values[0]\n",
    "\n",
    "#extract the column of this maximum company from the data frame\n",
    "maximum_company_data = data[maximum_company]\n",
    "\n",
    "#initialize empty arrays for inputs and output\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "#loop over the whole column\n",
    "#in each iteration append 5 values representing days from (t-4 till t) to the input array\n",
    "#and the following 4 values representing days (t+1 till t+4) to the output array\n",
    "#shift the dataframe to the left by 1 step (-1) to take following values\n",
    "for value in range(0,len(maximum_company_data)-9):\n",
    "    x_element = list(maximum_company_data[:5])\n",
    "    y_element = list(maximum_company_data[5:9])\n",
    "    X.append(x_element)\n",
    "    Y.append(y_element)\n",
    "    maximum_company_data = maximum_company_data.shift(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'hidden_layer_sizes': (10, 2), 'solver': 'sgd'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.974538146972305"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing Data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "Y_scaled = preprocessing.scale(Y)\n",
    "\n",
    "#divide the dataset into training and testing sets with ratio 80:20\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_scaled,Y_scaled,test_size=0.2)\n",
    "\n",
    "#define the MLP regressor with maximum iterations\n",
    "regressor = MLPRegressor(max_iter=3000)\n",
    "\n",
    "#specifying the different parameters to perform the grid search on\n",
    "param_grid = {'hidden_layer_sizes': [(15, 2), (10, 2)], 'activation': ['relu', 'tanh'], 'solver':['sgd', 'adam']}\n",
    "\n",
    "#perform gridsearch on all possible combinations of parameters with cross validation division\n",
    "clf = GridSearchCV(regressor, param_grid=param_grid, cv=3)\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "#print the best architecture parameters and the score\n",
    "print(clf.best_params_)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
